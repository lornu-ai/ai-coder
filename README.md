# ai-coder

[![CI](https://github.com/lornu-ai/ai-coder/actions/workflows/ci.yml/badge.svg)](https://github.com/lornu-ai/ai-coder/actions/workflows/ci.yml)
[![Security](https://github.com/lornu-ai/ai-coder/actions/workflows/security.yml/badge.svg)](https://github.com/lornu-ai/ai-coder/actions/workflows/security.yml)

A blazingly fast CLI tool for AI-assisted coding using local Ollama models on your GPU.

> **New to ai-coder?** üëâ **[Start with the SETUP.md guide](./SETUP.md)** - Complete step-by-step instructions

## Features

- üöÄ **Fast Local Inference**: Run large language models directly on your GPU with Ollama
- üîê **Privacy-First**: All processing happens locally‚Äîno data sent to external APIs
- ‚ö° **Streaming Output**: Real-time streaming responses as they're generated
- ü§ñ **Agent Mode**: Automatically extract and execute bash commands from LLM output
- üîß **Configurable**: Choose your model and Ollama instance with ease
- üí∞ **Free Forever**: No API costs, subscriptions, or limits

## Prerequisites

1. **Ollama** installed and running locally
   ```bash
   ollama serve
   ```

2. **A coding model** pulled (e.g., qwen2.5-coder, deepseek-coder-v2)
   ```bash
   ollama pull qwen2.5-coder
   ```

3. **Rust 1.70+** (for building from source)

## Installation

Clone and build:

```bash
git clone https://github.com/lornu-ai/ai-coder.git
cd ai-coder
cargo build --release
```

The binary will be available at `./target/release/ai-coder`.

## Usage

### Basic Example

```bash
./target/release/ai-coder "Write a fast Fibonacci sequence generator in Rust"
```

### Specify a Different Model

```bash
./target/release/ai-coder --model deepseek-coder-v2 "Implement a binary search algorithm"
```

### Custom Ollama Host

```bash
# Via command-line flag
./target/release/ai-coder -H http://192.168.1.50:11434 "Your prompt here"

# Via environment variable
OLLAMA_HOST="http://192.168.1.50:11434" ./target/release/ai-coder "Your prompt here"
```

### Agent Mode (Execute Bash Commands)

‚ö†Ô∏è **SECURITY WARNING**: Agent mode executes bash commands generated by the LLM. This carries inherent risks from prompt injection attacks. **Only use agent mode with trusted prompts and in safe environments.** Never use `--yes` flag in production or with untrusted inputs.

Enable agent mode to automatically extract and execute bash commands from the model's output:

```bash
# Interactive mode - prompts before executing commands (recommended)
./target/release/ai-coder --agent "create a new git repository and add a README"

# Auto-approve mode - executes without confirmation (‚ö†Ô∏è use with extreme caution)
./target/release/ai-coder --agent --yes "install dependencies with npm"
```

#### How Agent Mode Works

1. **Generation**: Model generates a response with bash code blocks (triple backticks)
2. **Extraction**: ai-coder automatically parses and identifies bash/sh code blocks
3. **Approval**: Shows extracted commands and prompts for confirmation (unless `--yes` is used)
4. **Execution**: Safely executes approved commands with output capture
5. **Feedback**: Displays execution status with ‚úì (success) or ‚ö†Ô∏è (failure) indicators

#### When Agent Mode Works Best

Agent mode is most effective for tasks that naturally involve bash commands:

‚úÖ **Good for agent mode:**
- File operations: `mkdir`, `touch`, `cp`, `rm`, `mv`
- Git workflows: `git init`, `git add`, `git commit`, `git push`
- Package management: `npm install`, `cargo build`, `pip install`
- Script generation and execution
- Directory navigation and exploration

‚ùå **Not suitable for agent mode:**
- GitHub API operations (posting reviews, managing PRs via web)
- High-level tasks without clear bash equivalents
- Tasks requiring interactive user input or browser automation
- Complex decision-making or multi-step workflows

**Tip**: Use agent mode for technical tasks with clear bash command solutions. For other tasks, use regular chat mode and copy-paste the generated code manually.

#### Example Output

```bash
$ ./target/release/ai-coder --agent "create a git repo and add a file"

[ai-coder] Mode: AGENT
[ai-coder] Using model: qwen2.5-coder
...
<LLM generates code with bash blocks>

[ai-coder-agent] Found bash command(s):
============================================================
mkdir my-project
cd my-project
git init
echo "# My Project" > README.md
git add .
git commit -m "Initial commit"
============================================================

[ai-coder-agent] Execute? (y/n): y

[ai-coder-agent] Executing...
[ai-coder-agent] ‚úì Command succeeded
```

### Full Options

```bash
./target/release/ai-coder --help
```

## Architecture

- **Language**: Rust (async/await with Tokio)
- **HTTP Client**: Reqwest with streaming support
- **CLI Framework**: Clap for command-line argument parsing
- **Ollama Integration**: Local REST API calls to localhost:11434

## How It Works

1. Takes your prompt as a CLI argument
2. Connects to your local Ollama instance (default: http://localhost:11434)
3. Sends a streaming request to the model
4. Streams the output directly to your terminal in real-time
5. Exits when generation is complete

## Configuration

### Environment Variables

- `OLLAMA_HOST`: Default Ollama instance URL (e.g., `http://localhost:11434`)

### Command-Line Flags

- `-m, --model <MODEL>`: Model name (default: `qwen2.5-coder`)
- `-H, --host <HOST>`: Ollama host URL (overrides `OLLAMA_HOST` env var)
- `-a, --agent`: Enable agent mode - automatically extract and execute bash commands
- `-y, --yes`: Auto-approve commands in agent mode without confirmation

## Security Considerations

### Agent Mode Risks

Agent mode executes bash commands generated by LLMs. This is inherently risky due to:

1. **Prompt Injection**: Attackers can craft inputs that make the LLM generate malicious commands
2. **Unpredictable Output**: LLMs can generate unexpected or harmful bash commands
3. **No Sandboxing**: Commands execute with your user's full permissions

### Best Practices

- ‚úÖ **Always use interactive mode** (without `--yes`) to review commands before execution
- ‚úÖ **Use agent mode only in safe, isolated environments** (VMs, containers, throwaway systems)
- ‚úÖ **Verify all commands** before confirming execution
- ‚úÖ **Limit permissions**: Run ai-coder with minimal necessary permissions
- ‚úÖ **Use specific prompts**: Be explicit about what you want (avoids ambiguity)
- ‚ùå **Never use `--yes` flag** with untrusted or adversarial prompts
- ‚ùå **Never use agent mode in production systems** or critical environments
- ‚ùå **Don't pipe untrusted input** to ai-coder

### Recommended Setup

```bash
# Use in a dedicated directory with limited permissions
mkdir ~/ai-coder-sandbox
cd ~/ai-coder-sandbox

# Run ai-coder in interactive mode (with confirmation prompts)
~/target/release/ai-coder --agent "your trusted prompt here"
# Review the commands before confirming execution
```

## Performance Tips

1. **GPU VRAM**: Models typically require 6-14GB VRAM. Check your GPU capacity.
2. **Model Selection**: Start with smaller models (7B) for faster iterations.
3. **Temperature**: For coding, lower temperature values produce more deterministic output.
4. **Context Length**: Larger context windows allow for more complex prompts.

## Roadmap

- [x] Agent mode support (auto-execute bash commands) - **Completed v0.1.0**
- [ ] Project file context integration
- [ ] Configuration file support
- [ ] Multi-turn conversation mode
- [ ] Code formatting and syntax highlighting
- [ ] Interactive REPL mode for multi-step workflows

## License

MIT

## CI/CD & Releases

### Automated Testing & Building
Every push and pull request runs through our automated CI pipeline:
- ‚úÖ Format checking (`cargo fmt`)
- ‚úÖ Linting (`cargo clippy`)
- ‚úÖ Unit tests (`cargo test`)
- ‚úÖ Release builds on Linux, macOS, Windows
- ‚úÖ Security audits (dependency vulnerabilities)
- ‚úÖ Nix reproducible builds

See [.github/WORKFLOWS.md](.github/WORKFLOWS.md) for details.

### Releases
Binary releases are automatically created when you push a version tag:
```bash
git tag v0.2.0
git push origin v0.2.0
```

This triggers builds for all platforms and creates a GitHub release with downloadable binaries.

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch from `develop`
3. Make your changes
4. Push and open a PR against `develop`
5. Our CI will automatically test your changes

The CI checks must pass before merging.

## Support

For Ollama issues: https://github.com/ollama/ollama
For ai-coder issues: https://github.com/lornu-ai/ai-coder/issues
